{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "apparent-cookbook",
   "metadata": {},
   "source": [
    "# Homework Assignment 6: Model Evaluation 2\n",
    "As in the previous assignments, in this homework assignment you will continue your exploration of the [SWAN-SF Dataset](https://doi.org/10.7910/DVN/EBCFKM), described in the paper found [here](https://doi.org/10.1038/s41597-020-0548-x).\n",
    "\n",
    "This assignment will continue to utilize a copy of the extracted feature dataset we used in Homework 5. Recall that the dataset has been processed by performing log, z-score and range scaling. We continuing to use more than one partition worth of data, so for the scaling, the mean, standard deviation, minimum, and maximum were calculated using data from both partitions so that a global scaling can be performed on each partition. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-portugal",
   "metadata": {},
   "source": [
    "## Step 1: Downloading the Data\n",
    "\n",
    "This assignment will continue to use [Partition 1](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/BMXYCB) for a training set and [Partition 2](https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/TCRPUD) as a testing set. \n",
    "\n",
    "---\n",
    "\n",
    "For this assignment, cleaning, transforming, and normalization of the data has been completed using both partitions to find the various minimum, maximum, standard deviation, and mean values needed to perform these operations. Recall from lecture that we should not perform these operations on each partition individually, but as a whole as there may(will) be different values for these in different partitions. \n",
    "\n",
    "For example, if we perform simple range scaling on each partition individually and we see a range of 0 to 100 in one partition and 0 to 10 in another. After individual scaling the values with 100 in the first would be mapped to 1 just like the values that had 10 in the second. This can cause serious performance problems in your model, so I have made sure that the normalization was treated properly for you. \n",
    "\n",
    "Below you will find the full partitions and `toy` sampled data from each partition, where only 20 samples from each of our 5 classes have been included in the data.  \n",
    "\n",
    "#### Full\n",
    "- [Full Normalized Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/normalized_partition1ExtractedFeatures.csv)\n",
    "- [Full Normalized Partition 2 feature dataset](http://dmlab.cs.gsu.edu/solar/data/normalized_partition2ExtractedFeatures.csv)\n",
    "\n",
    "#### Toy\n",
    "- [Toy Normalized Partition 1 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_normalized_partition1ExtractedFeatures.csv)\n",
    "- [Toy Normalized Partition 2 feature dataset](http://dmlab.cs.gsu.edu/solar/data/toy_normalized_partition2ExtractedFeatures.csv)\n",
    "\n",
    "Now that you have the two files, you should load each into a Pandas DataFrame using the [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-spare",
   "metadata": {},
   "source": [
    "### Evaluation Metric\n",
    "\n",
    "As was done in Homework 5, for each of the models we evaluate in this assignmnet, you will calculate the True Skill Statistic score using the test data from Partition 2 to determine which model performs the best for classifying the positive flaring class.\n",
    "\n",
    "    True skill statistic (TSS) = TPR + TNR - 1 = TPR - (1-TNR) = TPR - FPR\n",
    "\n",
    "Where:\n",
    "\n",
    "    True positive rate (TPR) = TP/(TP+FN) Also known as recall or sensitivity\n",
    "    True negative rate (TNR) = TN/(TN+FP) Also known as specificity or selectivity\n",
    "    False positive rate (FPR) = FP/(FP+TN) = (1-TNR) Also known as fall-out or false alarm ratio\n",
    "\n",
    "\n",
    "**Recall**\n",
    "\n",
    "    True positive (TP)\n",
    "    True negative (TN)\n",
    "    False positive (FP)\n",
    "    False negative (FN)\n",
    "    \n",
    "See [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for more information.\n",
    "\n",
    "Below is a function implemented to provide your score for each model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "individual-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from pandas import DataFrame \n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facial-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tss(y_true, y_predict):\n",
    "    '''\n",
    "    Calculates the true skill score for binary classification based on the output of the confusion\n",
    "    table function\n",
    "    \n",
    "        Parameters:\n",
    "            y_true   : A vector/list of values that represent the true class label of the data being evaluated.\n",
    "            y_predict: A vector/list of values that represent the predicted class label for the data being evaluated.\n",
    "    \n",
    "        Returns:\n",
    "            tss_value (float): A floating point value (-1.0,1.0) indicating the TSS of the input data\n",
    "    '''\n",
    "    scores = confusion_matrix(y_true, y_predict).ravel()\n",
    "    TN, FP, FN, TP = scores\n",
    "    print('TN={0}\\tFP={1}\\tFN={2}\\tTP={3}'.format(TN, FP, FN, TP))\n",
    "    tp_rate = TP / float(TP + FN) if TP > 0 else 0  \n",
    "    fp_rate = FP / float(FP + TN) if FP > 0 else 0\n",
    "    \n",
    "    return tp_rate - fp_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04d14e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In addition to the TSS, you will be asked to also calculate the Heidke Skill Score (HSS) to see how much better your model performs than a random forecast.  \n",
    "\n",
    "Below is a function implemented to provide your score fore each model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab05f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hss(y_true, y_predict):\n",
    "    '''\n",
    "    Calculates the Heidke Skill Score for binary classification based on the output of the confusion\n",
    "    table function.\n",
    "    \n",
    "    The HSS measures the fractional improvement of the forecast over the standard forecast.\n",
    "    The \"standard forecast\" is usually the number correct by chance or the proportion \n",
    "    correct by chance.\n",
    "    \n",
    "        Parameters:\n",
    "            y_true   : A vector/list of values that represent the true class label of the data being evaluated.\n",
    "            y_predict: A vector/list of values that represent the predicted class label for the data being evaluated.\n",
    "    \n",
    "        Returns:\n",
    "            hss_value (float): A floating point value (-inf,1.0) indicating the HSS of the input data. \n",
    "                Negative values indicate that the chance forecast is better, 0 means no skill, and a perfect forecast obtains a HSS of 1.\n",
    "    '''\n",
    "    scores = confusion_matrix(y_true, y_predict).ravel()\n",
    "    TN, FP, FN, TP = scores\n",
    "    #print('TN={0}\\tFP={1}\\tFN={2}\\tTP={3}'.format(TN, FP, FN, TP))\n",
    "    P = float(TP + FN)\n",
    "    N = float(FP + TN)\n",
    "    numerator = 2*((TP * TN) - (FN * FP))\n",
    "    denominator = P*(FN + TN) + N*(TP + FP)\n",
    "    \n",
    "    return numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-alfred",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As in the previous assignment, we will be utilizing a binary classification of our 5 class dataset. So, below is the helper function to change our class labels from the 5 class target feature to the binary target feature. The function is implemented to take a dataframe (e.g. our `abt`) and prepares it for a binary classification by merging the `X`- and `M`-class samples into one group, and the rest (`NF`, `B`, and `C`) into another group, labeled with `1`s and `0`s, respectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "handled-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dichotomize_X_y(data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    dichotomizes the dataset and split it into the features (X) and the labels (y).\n",
    "    \n",
    "    :return: two np.ndarray objects X and y.\n",
    "    \"\"\"\n",
    "    data_dich = data.copy()\n",
    "    data_dich['lab'] = data_dich['lab'].map({'NF': 0, 'B': 0, 'C': 0, 'M': 1, 'X': 1})\n",
    "    y = data_dich['lab']\n",
    "    X = data_dich.drop(['lab'], axis=1)\n",
    "    return X.values, y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-manufacturer",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Reading the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "viral-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/zeek/Desktop/'\n",
    "data_file = \"normalized_partition1ExtractedFeatures.csv\"\n",
    "data_file2 = \"normalized_partition2ExtractedFeatures.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "infectious-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "abt = pd.read_csv(os.path.join(data_dir, data_file))\n",
    "abt2 = pd.read_csv(os.path.join(data_dir, data_file2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78989fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Run Feature Selection\n",
    "\n",
    "Below you have code to perform feature selction using [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). The scoring function being used is [scikit-learn f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif).\n",
    "\n",
    "Once feature selection is done with this one method, a set of training and testing dataframes are constructed by doing the following:\n",
    "\n",
    "* Utilizing the `get_support` function of the feature selection object we get a mask of the features we will select from our original analytics base table DataFrame.  \n",
    "\n",
    "* The mask of selected features is then paired with the `loc` function on our datframe containing only the descriptive features to get our selected featrues on all rows in our feature dataframe.\n",
    "\n",
    "* The set of selected features are concatenated with our labels to construct a training dataset.\n",
    "\n",
    "* This process was then repeated to construct the testing set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "818faa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20\n",
    "\n",
    "# Split the target and descriptive features for Partition 1 into two \n",
    "# different DataFrame objects\n",
    "df_labels = abt['lab'].copy()\n",
    "df_feats = abt.copy().drop(['lab'], axis=1)\n",
    "\n",
    "# Split the target and descriptive features for Partition 2 inot two\n",
    "# different DataFrame Objects\n",
    "df_test_labels = abt2['lab'].copy()\n",
    "df_test_feats = abt2.copy().drop(['lab'], axis=1)\n",
    "\n",
    "# Do feature selection\n",
    "feats1 = SelectKBest(f_classif, k=numFeat).fit(df_feats, df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "041022de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a training dataset from Partition 1 with only the selected descriptive \n",
    "# features and the target feature\n",
    "df_selected_feats1 = df_feats.loc[:, feats1.get_support()]\n",
    "df_train_set1 = pd.concat([df_labels, df_selected_feats1], axis=1)\n",
    "\n",
    "# Construct a testing dataset from Partition 2 with only the selected descriptive\n",
    "# features and the target feature\n",
    "df_test_selected_feats1 = df_test_feats.loc[:, feats1.get_support()]\n",
    "df_test_set1 = pd.concat([df_test_labels, df_test_selected_feats1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28981223",
   "metadata": {},
   "source": [
    "---\n",
    "### Q1 (5 points)\n",
    "\n",
    "Using the feature selection task above as a template, you will now perform feature selection again to produce a second set of training and testing data. This time you will use the [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). \n",
    "\n",
    "Instead of using the [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) that is shown in the example documentation linked above, you will be utilizing the [LassoLars](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars) as your `estimator`.  You should also set the `max_features` to the number of features we are going to select (20 features). \n",
    "\n",
    "**Note:** The LassoLars class is a regression model and will not work on our string class lables. So, you need to map the `NF`, `B`, `C`, `M`, `X` labels to a range between `-1` and `1` before you attempt to fit the feature selection model to them. You can try evenly spacing them, or forcing the classes that will be used as the negative class into a tight range and those that will be used as the postive class into another tight range.  Maybe this another location for you to do hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28515ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20\n",
    "abt_copy = abt.copy()\n",
    "abt2_copy = abt2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbe2d3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTBSQ_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTUSJZ_dderivative_mean</th>\n",
       "      <th>TOTUSJZ_gderivative_mean</th>\n",
       "      <th>SAVNCPP_gderivative_mean</th>\n",
       "      <th>ABSNJZH_kurtosis</th>\n",
       "      <th>TOTUSJH_gderivative_mean</th>\n",
       "      <th>ABSNJZH_stddev</th>\n",
       "      <th>MEANPOT_slope_of_longest_mono_decrease</th>\n",
       "      <th>MEANSHR_slope_of_longest_mono_decrease</th>\n",
       "      <th>...</th>\n",
       "      <th>SHRGT45_slope_of_longest_mono_decrease</th>\n",
       "      <th>EPSX_slope_of_longest_mono_decrease</th>\n",
       "      <th>USFLUX_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTBSQ_gderivative_stddev</th>\n",
       "      <th>TOTPOT_average_absolute_change</th>\n",
       "      <th>TOTBSQ_average_absolute_change</th>\n",
       "      <th>MEANPOT_difference_of_medians</th>\n",
       "      <th>USFLUX_slope_of_longest_mono_increase</th>\n",
       "      <th>TOTBSQ_avg_mono_increase_slope</th>\n",
       "      <th>SAVNCPP_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>0.979145</td>\n",
       "      <td>0.979265</td>\n",
       "      <td>0.975866</td>\n",
       "      <td>0.046063</td>\n",
       "      <td>0.683983</td>\n",
       "      <td>0.166475</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998884</td>\n",
       "      <td>0.999664</td>\n",
       "      <td>0.953375</td>\n",
       "      <td>0.015288</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.026092</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.028674</td>\n",
       "      <td>0.021381</td>\n",
       "      <td>0.073789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.979309</td>\n",
       "      <td>0.979332</td>\n",
       "      <td>0.977080</td>\n",
       "      <td>0.030247</td>\n",
       "      <td>0.682688</td>\n",
       "      <td>0.122735</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.998197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998806</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.009592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.979237</td>\n",
       "      <td>0.979280</td>\n",
       "      <td>0.976580</td>\n",
       "      <td>0.047912</td>\n",
       "      <td>0.680145</td>\n",
       "      <td>0.105429</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999086</td>\n",
       "      <td>0.996353</td>\n",
       "      <td>0.998553</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.007625</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>0.017374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>0.979829</td>\n",
       "      <td>0.979674</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>0.038173</td>\n",
       "      <td>0.694549</td>\n",
       "      <td>0.183616</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.999569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999646</td>\n",
       "      <td>0.999499</td>\n",
       "      <td>0.996641</td>\n",
       "      <td>0.014514</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.028324</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.016026</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>0.037189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.979351</td>\n",
       "      <td>0.976705</td>\n",
       "      <td>0.047934</td>\n",
       "      <td>0.684250</td>\n",
       "      <td>0.103106</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.997813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.997705</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.008830</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.007592</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.018851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73487</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>0.979317</td>\n",
       "      <td>0.979341</td>\n",
       "      <td>0.976041</td>\n",
       "      <td>0.052463</td>\n",
       "      <td>0.684865</td>\n",
       "      <td>0.057351</td>\n",
       "      <td>0.999655</td>\n",
       "      <td>0.999328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998228</td>\n",
       "      <td>0.997778</td>\n",
       "      <td>0.997607</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.010243</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.005171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73488</th>\n",
       "      <td>NF</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.979596</td>\n",
       "      <td>0.979535</td>\n",
       "      <td>0.975673</td>\n",
       "      <td>0.039513</td>\n",
       "      <td>0.696048</td>\n",
       "      <td>0.136841</td>\n",
       "      <td>0.999709</td>\n",
       "      <td>0.996946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997911</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>0.992283</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.012674</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.009679</td>\n",
       "      <td>0.014201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73489</th>\n",
       "      <td>C</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.979785</td>\n",
       "      <td>0.979718</td>\n",
       "      <td>0.979449</td>\n",
       "      <td>0.063704</td>\n",
       "      <td>0.695232</td>\n",
       "      <td>0.214363</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>0.973915</td>\n",
       "      <td>0.025836</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.049894</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.019388</td>\n",
       "      <td>0.022464</td>\n",
       "      <td>0.057426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73490</th>\n",
       "      <td>B</td>\n",
       "      <td>0.999581</td>\n",
       "      <td>0.978214</td>\n",
       "      <td>0.978635</td>\n",
       "      <td>0.978052</td>\n",
       "      <td>0.048476</td>\n",
       "      <td>0.659593</td>\n",
       "      <td>0.184096</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.999761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999053</td>\n",
       "      <td>0.999981</td>\n",
       "      <td>0.994456</td>\n",
       "      <td>0.020409</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.035119</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.024067</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.043784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73491</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.979405</td>\n",
       "      <td>0.979393</td>\n",
       "      <td>0.975928</td>\n",
       "      <td>0.063802</td>\n",
       "      <td>0.687184</td>\n",
       "      <td>0.044799</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.993584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999607</td>\n",
       "      <td>0.992271</td>\n",
       "      <td>0.999920</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.002329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73492 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lab  TOTBSQ_slope_of_longest_mono_decrease  TOTUSJZ_dderivative_mean  \\\n",
       "0      NF                               0.999113                  0.979145   \n",
       "1      NF                               0.999991                  0.979309   \n",
       "2      NF                               0.999990                  0.979237   \n",
       "3      NF                               0.999751                  0.979829   \n",
       "4      NF                               0.999948                  0.979352   \n",
       "...    ..                                    ...                       ...   \n",
       "73487  NF                               0.999872                  0.979317   \n",
       "73488  NF                               1.000000                  0.979596   \n",
       "73489   C                               0.999981                  0.979785   \n",
       "73490   B                               0.999581                  0.978214   \n",
       "73491  NF                               0.999994                  0.979405   \n",
       "\n",
       "       TOTUSJZ_gderivative_mean  SAVNCPP_gderivative_mean  ABSNJZH_kurtosis  \\\n",
       "0                      0.979265                  0.975866          0.046063   \n",
       "1                      0.979332                  0.977080          0.030247   \n",
       "2                      0.979280                  0.976580          0.047912   \n",
       "3                      0.979674                  0.975500          0.038173   \n",
       "4                      0.979351                  0.976705          0.047934   \n",
       "...                         ...                       ...               ...   \n",
       "73487                  0.979341                  0.976041          0.052463   \n",
       "73488                  0.979535                  0.975673          0.039513   \n",
       "73489                  0.979718                  0.979449          0.063704   \n",
       "73490                  0.978635                  0.978052          0.048476   \n",
       "73491                  0.979393                  0.975928          0.063802   \n",
       "\n",
       "       TOTUSJH_gderivative_mean  ABSNJZH_stddev  \\\n",
       "0                      0.683983        0.166475   \n",
       "1                      0.682688        0.122735   \n",
       "2                      0.680145        0.105429   \n",
       "3                      0.694549        0.183616   \n",
       "4                      0.684250        0.103106   \n",
       "...                         ...             ...   \n",
       "73487                  0.684865        0.057351   \n",
       "73488                  0.696048        0.136841   \n",
       "73489                  0.695232        0.214363   \n",
       "73490                  0.659593        0.184096   \n",
       "73491                  0.687184        0.044799   \n",
       "\n",
       "       MEANPOT_slope_of_longest_mono_decrease  \\\n",
       "0                                    0.999915   \n",
       "1                                    0.999982   \n",
       "2                                    0.999988   \n",
       "3                                    0.999948   \n",
       "4                                    0.999954   \n",
       "...                                       ...   \n",
       "73487                                0.999655   \n",
       "73488                                0.999709   \n",
       "73489                                0.999804   \n",
       "73490                                0.999951   \n",
       "73491                                0.999985   \n",
       "\n",
       "       MEANSHR_slope_of_longest_mono_decrease  ...  \\\n",
       "0                                    0.998414  ...   \n",
       "1                                    0.998197  ...   \n",
       "2                                    0.999814  ...   \n",
       "3                                    0.999569  ...   \n",
       "4                                    0.997813  ...   \n",
       "...                                       ...  ...   \n",
       "73487                                0.999328  ...   \n",
       "73488                                0.996946  ...   \n",
       "73489                                0.999597  ...   \n",
       "73490                                0.999761  ...   \n",
       "73491                                0.993584  ...   \n",
       "\n",
       "       SHRGT45_slope_of_longest_mono_decrease  \\\n",
       "0                                    0.998884   \n",
       "1                                    0.998806   \n",
       "2                                    0.999086   \n",
       "3                                    0.999646   \n",
       "4                                    0.999126   \n",
       "...                                       ...   \n",
       "73487                                0.998228   \n",
       "73488                                0.997911   \n",
       "73489                                0.999917   \n",
       "73490                                0.999053   \n",
       "73491                                0.999607   \n",
       "\n",
       "       EPSX_slope_of_longest_mono_decrease  \\\n",
       "0                                 0.999664   \n",
       "1                                 0.998982   \n",
       "2                                 0.996353   \n",
       "3                                 0.999499   \n",
       "4                                 0.999903   \n",
       "...                                    ...   \n",
       "73487                             0.997778   \n",
       "73488                             0.999393   \n",
       "73489                             0.999618   \n",
       "73490                             0.999981   \n",
       "73491                             0.992271   \n",
       "\n",
       "       USFLUX_slope_of_longest_mono_decrease  TOTBSQ_gderivative_stddev  \\\n",
       "0                                   0.953375                   0.015288   \n",
       "1                                   0.999580                   0.002713   \n",
       "2                                   0.998553                   0.004391   \n",
       "3                                   0.996641                   0.014514   \n",
       "4                                   0.997705                   0.004602   \n",
       "...                                      ...                        ...   \n",
       "73487                               0.997607                   0.002863   \n",
       "73488                               0.992283                   0.005927   \n",
       "73489                               0.973915                   0.025836   \n",
       "73490                               0.994456                   0.020409   \n",
       "73491                               0.999920                   0.000939   \n",
       "\n",
       "       TOTPOT_average_absolute_change  TOTBSQ_average_absolute_change  \\\n",
       "0                            0.000074                        0.026092   \n",
       "1                            0.000017                        0.005966   \n",
       "2                            0.000032                        0.009646   \n",
       "3                            0.000147                        0.028324   \n",
       "4                            0.000045                        0.008830   \n",
       "...                               ...                             ...   \n",
       "73487                        0.000015                        0.005738   \n",
       "73488                        0.000049                        0.012674   \n",
       "73489                        0.000237                        0.049894   \n",
       "73490                        0.000103                        0.035119   \n",
       "73491                        0.000004                        0.002307   \n",
       "\n",
       "       MEANPOT_difference_of_medians  USFLUX_slope_of_longest_mono_increase  \\\n",
       "0                           0.000041                               0.028674   \n",
       "1                           0.000035                               0.003590   \n",
       "2                           0.000003                               0.007625   \n",
       "3                           0.000033                               0.016026   \n",
       "4                           0.000005                               0.007592   \n",
       "...                              ...                                    ...   \n",
       "73487                       0.000006                               0.010243   \n",
       "73488                       0.000217                               0.005476   \n",
       "73489                       0.000085                               0.019388   \n",
       "73490                       0.000019                               0.024067   \n",
       "73491                       0.000018                               0.001562   \n",
       "\n",
       "       TOTBSQ_avg_mono_increase_slope  SAVNCPP_max  \n",
       "0                            0.021381     0.073789  \n",
       "1                            0.005345     0.009592  \n",
       "2                            0.007204     0.017374  \n",
       "3                            0.021110     0.037189  \n",
       "4                            0.006826     0.018851  \n",
       "...                               ...          ...  \n",
       "73487                        0.005423     0.005171  \n",
       "73488                        0.009679     0.014201  \n",
       "73489                        0.022464     0.057426  \n",
       "73490                        0.020761     0.043784  \n",
       "73491                        0.001973     0.002329  \n",
       "\n",
       "[73492 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #----------------------------------------------\n",
    "    l_labs = abt_copy['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "    l_feats = abt_copy.drop(['lab'], axis=1)\n",
    "    l_dffeats =   SelectFromModel(max_features = numFeat, estimator = LassoLars( alpha = 0, eps =1)).fit(l_feats, l_labs)\n",
    "    l_train = l_feats.loc[:, l_dffeats.get_support()]  \n",
    "    l_trainset = pd.concat([abt['lab'], l_train], axis = 1)\n",
    "    l_trainset\n",
    "    #----------------------------------------------\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502e4a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTBSQ_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTUSJZ_dderivative_mean</th>\n",
       "      <th>TOTUSJZ_gderivative_mean</th>\n",
       "      <th>SAVNCPP_gderivative_mean</th>\n",
       "      <th>ABSNJZH_kurtosis</th>\n",
       "      <th>TOTUSJH_gderivative_mean</th>\n",
       "      <th>ABSNJZH_stddev</th>\n",
       "      <th>MEANPOT_slope_of_longest_mono_decrease</th>\n",
       "      <th>MEANSHR_slope_of_longest_mono_decrease</th>\n",
       "      <th>...</th>\n",
       "      <th>SHRGT45_slope_of_longest_mono_decrease</th>\n",
       "      <th>EPSX_slope_of_longest_mono_decrease</th>\n",
       "      <th>USFLUX_slope_of_longest_mono_decrease</th>\n",
       "      <th>TOTBSQ_gderivative_stddev</th>\n",
       "      <th>TOTPOT_average_absolute_change</th>\n",
       "      <th>TOTBSQ_average_absolute_change</th>\n",
       "      <th>MEANPOT_difference_of_medians</th>\n",
       "      <th>USFLUX_slope_of_longest_mono_increase</th>\n",
       "      <th>TOTBSQ_avg_mono_increase_slope</th>\n",
       "      <th>SAVNCPP_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999113</td>\n",
       "      <td>0.979145</td>\n",
       "      <td>0.979265</td>\n",
       "      <td>0.975866</td>\n",
       "      <td>0.046063</td>\n",
       "      <td>0.683983</td>\n",
       "      <td>0.166475</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.998414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998884</td>\n",
       "      <td>0.999664</td>\n",
       "      <td>0.953375</td>\n",
       "      <td>0.015288</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.026092</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.028674</td>\n",
       "      <td>0.021381</td>\n",
       "      <td>0.073789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.979309</td>\n",
       "      <td>0.979332</td>\n",
       "      <td>0.977080</td>\n",
       "      <td>0.030247</td>\n",
       "      <td>0.682688</td>\n",
       "      <td>0.122735</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.998197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998806</td>\n",
       "      <td>0.998982</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.009592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.979237</td>\n",
       "      <td>0.979280</td>\n",
       "      <td>0.976580</td>\n",
       "      <td>0.047912</td>\n",
       "      <td>0.680145</td>\n",
       "      <td>0.105429</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999086</td>\n",
       "      <td>0.996353</td>\n",
       "      <td>0.998553</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.009646</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.007625</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>0.017374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999751</td>\n",
       "      <td>0.979829</td>\n",
       "      <td>0.979674</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>0.038173</td>\n",
       "      <td>0.694549</td>\n",
       "      <td>0.183616</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.999569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999646</td>\n",
       "      <td>0.999499</td>\n",
       "      <td>0.996641</td>\n",
       "      <td>0.014514</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.028324</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.016026</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>0.037189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.979352</td>\n",
       "      <td>0.979351</td>\n",
       "      <td>0.976705</td>\n",
       "      <td>0.047934</td>\n",
       "      <td>0.684250</td>\n",
       "      <td>0.103106</td>\n",
       "      <td>0.999954</td>\n",
       "      <td>0.997813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999126</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.997705</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.008830</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.007592</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.018851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88552</th>\n",
       "      <td>NF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88553</th>\n",
       "      <td>NF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88554</th>\n",
       "      <td>NF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88555</th>\n",
       "      <td>NF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88556</th>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88557 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lab  TOTBSQ_slope_of_longest_mono_decrease  TOTUSJZ_dderivative_mean  \\\n",
       "0      NF                               0.999113                  0.979145   \n",
       "1      NF                               0.999991                  0.979309   \n",
       "2      NF                               0.999990                  0.979237   \n",
       "3      NF                               0.999751                  0.979829   \n",
       "4      NF                               0.999948                  0.979352   \n",
       "...    ..                                    ...                       ...   \n",
       "88552  NF                                    NaN                       NaN   \n",
       "88553  NF                                    NaN                       NaN   \n",
       "88554  NF                                    NaN                       NaN   \n",
       "88555  NF                                    NaN                       NaN   \n",
       "88556   B                                    NaN                       NaN   \n",
       "\n",
       "       TOTUSJZ_gderivative_mean  SAVNCPP_gderivative_mean  ABSNJZH_kurtosis  \\\n",
       "0                      0.979265                  0.975866          0.046063   \n",
       "1                      0.979332                  0.977080          0.030247   \n",
       "2                      0.979280                  0.976580          0.047912   \n",
       "3                      0.979674                  0.975500          0.038173   \n",
       "4                      0.979351                  0.976705          0.047934   \n",
       "...                         ...                       ...               ...   \n",
       "88552                       NaN                       NaN               NaN   \n",
       "88553                       NaN                       NaN               NaN   \n",
       "88554                       NaN                       NaN               NaN   \n",
       "88555                       NaN                       NaN               NaN   \n",
       "88556                       NaN                       NaN               NaN   \n",
       "\n",
       "       TOTUSJH_gderivative_mean  ABSNJZH_stddev  \\\n",
       "0                      0.683983        0.166475   \n",
       "1                      0.682688        0.122735   \n",
       "2                      0.680145        0.105429   \n",
       "3                      0.694549        0.183616   \n",
       "4                      0.684250        0.103106   \n",
       "...                         ...             ...   \n",
       "88552                       NaN             NaN   \n",
       "88553                       NaN             NaN   \n",
       "88554                       NaN             NaN   \n",
       "88555                       NaN             NaN   \n",
       "88556                       NaN             NaN   \n",
       "\n",
       "       MEANPOT_slope_of_longest_mono_decrease  \\\n",
       "0                                    0.999915   \n",
       "1                                    0.999982   \n",
       "2                                    0.999988   \n",
       "3                                    0.999948   \n",
       "4                                    0.999954   \n",
       "...                                       ...   \n",
       "88552                                     NaN   \n",
       "88553                                     NaN   \n",
       "88554                                     NaN   \n",
       "88555                                     NaN   \n",
       "88556                                     NaN   \n",
       "\n",
       "       MEANSHR_slope_of_longest_mono_decrease  ...  \\\n",
       "0                                    0.998414  ...   \n",
       "1                                    0.998197  ...   \n",
       "2                                    0.999814  ...   \n",
       "3                                    0.999569  ...   \n",
       "4                                    0.997813  ...   \n",
       "...                                       ...  ...   \n",
       "88552                                     NaN  ...   \n",
       "88553                                     NaN  ...   \n",
       "88554                                     NaN  ...   \n",
       "88555                                     NaN  ...   \n",
       "88556                                     NaN  ...   \n",
       "\n",
       "       SHRGT45_slope_of_longest_mono_decrease  \\\n",
       "0                                    0.998884   \n",
       "1                                    0.998806   \n",
       "2                                    0.999086   \n",
       "3                                    0.999646   \n",
       "4                                    0.999126   \n",
       "...                                       ...   \n",
       "88552                                     NaN   \n",
       "88553                                     NaN   \n",
       "88554                                     NaN   \n",
       "88555                                     NaN   \n",
       "88556                                     NaN   \n",
       "\n",
       "       EPSX_slope_of_longest_mono_decrease  \\\n",
       "0                                 0.999664   \n",
       "1                                 0.998982   \n",
       "2                                 0.996353   \n",
       "3                                 0.999499   \n",
       "4                                 0.999903   \n",
       "...                                    ...   \n",
       "88552                                  NaN   \n",
       "88553                                  NaN   \n",
       "88554                                  NaN   \n",
       "88555                                  NaN   \n",
       "88556                                  NaN   \n",
       "\n",
       "       USFLUX_slope_of_longest_mono_decrease  TOTBSQ_gderivative_stddev  \\\n",
       "0                                   0.953375                   0.015288   \n",
       "1                                   0.999580                   0.002713   \n",
       "2                                   0.998553                   0.004391   \n",
       "3                                   0.996641                   0.014514   \n",
       "4                                   0.997705                   0.004602   \n",
       "...                                      ...                        ...   \n",
       "88552                                    NaN                        NaN   \n",
       "88553                                    NaN                        NaN   \n",
       "88554                                    NaN                        NaN   \n",
       "88555                                    NaN                        NaN   \n",
       "88556                                    NaN                        NaN   \n",
       "\n",
       "       TOTPOT_average_absolute_change  TOTBSQ_average_absolute_change  \\\n",
       "0                            0.000074                        0.026092   \n",
       "1                            0.000017                        0.005966   \n",
       "2                            0.000032                        0.009646   \n",
       "3                            0.000147                        0.028324   \n",
       "4                            0.000045                        0.008830   \n",
       "...                               ...                             ...   \n",
       "88552                             NaN                             NaN   \n",
       "88553                             NaN                             NaN   \n",
       "88554                             NaN                             NaN   \n",
       "88555                             NaN                             NaN   \n",
       "88556                             NaN                             NaN   \n",
       "\n",
       "       MEANPOT_difference_of_medians  USFLUX_slope_of_longest_mono_increase  \\\n",
       "0                           0.000041                               0.028674   \n",
       "1                           0.000035                               0.003590   \n",
       "2                           0.000003                               0.007625   \n",
       "3                           0.000033                               0.016026   \n",
       "4                           0.000005                               0.007592   \n",
       "...                              ...                                    ...   \n",
       "88552                            NaN                                    NaN   \n",
       "88553                            NaN                                    NaN   \n",
       "88554                            NaN                                    NaN   \n",
       "88555                            NaN                                    NaN   \n",
       "88556                            NaN                                    NaN   \n",
       "\n",
       "       TOTBSQ_avg_mono_increase_slope  SAVNCPP_max  \n",
       "0                            0.021381     0.073789  \n",
       "1                            0.005345     0.009592  \n",
       "2                            0.007204     0.017374  \n",
       "3                            0.021110     0.037189  \n",
       "4                            0.006826     0.018851  \n",
       "...                               ...          ...  \n",
       "88552                             NaN          NaN  \n",
       "88553                             NaN          NaN  \n",
       "88554                             NaN          NaN  \n",
       "88555                             NaN          NaN  \n",
       "88556                             NaN          NaN  \n",
       "\n",
       "[88557 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #test\n",
    "    l2_labs = abt2_copy['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "    l2_feats = abt2_copy.drop(['lab'], axis=1)\n",
    "    l_df_testfeats = l2_feats.loc[:, l_dffeats.get_support()]\n",
    "    l_test = l_feats.loc[:, l_dffeats.get_support()]  \n",
    "    l_testset = pd.concat([abt2['lab'], l_test], axis = 1)\n",
    "    l_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a34b6f",
   "metadata": {},
   "source": [
    "---\n",
    "### Q2 (5 points)\n",
    "\n",
    "In this question, you will again perform the feature selection task [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). However, this time, you will be utililizing a random forest model called [ExtraTressClssifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) as the `estimator`. \n",
    "\n",
    "You need set the `max_features` of the SelectFromModel object to the number of features we are going to select (20 features). \n",
    "\n",
    "You also need to set the `n_estimators` of the random forest algorithm to `75` when you construct it.\n",
    "\n",
    "**Note:** This method allows you to utilize our string class labels, so you don't need to map the lables to any other values. You can use the labels that were used in the original example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4deb923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeat = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e9454a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTUSJH_var</th>\n",
       "      <th>TOTBSQ_max</th>\n",
       "      <th>TOTBSQ_quadratic_weighted_average</th>\n",
       "      <th>TOTBSQ_last_value</th>\n",
       "      <th>TOTPOT_stddev</th>\n",
       "      <th>TOTPOT_linear_weighted_average</th>\n",
       "      <th>TOTUSJZ_mean</th>\n",
       "      <th>USFLUX_quadratic_weighted_average</th>\n",
       "      <th>R_VALUE_median</th>\n",
       "      <th>...</th>\n",
       "      <th>R_VALUE_linear_weighted_average</th>\n",
       "      <th>R_VALUE_quadratic_weighted_average</th>\n",
       "      <th>R_VALUE_last_value</th>\n",
       "      <th>TOTUSJH_min</th>\n",
       "      <th>TOTUSJH_max</th>\n",
       "      <th>TOTUSJH_quadratic_weighted_average</th>\n",
       "      <th>TOTUSJH_last_value</th>\n",
       "      <th>ABSNJZH_dderivative_stddev</th>\n",
       "      <th>ABSNJZH_avg_mono_increase_slope</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.703435</td>\n",
       "      <td>0.886776</td>\n",
       "      <td>0.880515</td>\n",
       "      <td>0.883499</td>\n",
       "      <td>0.856244</td>\n",
       "      <td>0.891949</td>\n",
       "      <td>0.930756</td>\n",
       "      <td>0.950338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086008</td>\n",
       "      <td>0.095111</td>\n",
       "      <td>0.371876</td>\n",
       "      <td>0.238758</td>\n",
       "      <td>0.275990</td>\n",
       "      <td>0.250146</td>\n",
       "      <td>0.271143</td>\n",
       "      <td>0.143979</td>\n",
       "      <td>0.147190</td>\n",
       "      <td>0.294529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.536687</td>\n",
       "      <td>0.837925</td>\n",
       "      <td>0.829410</td>\n",
       "      <td>0.828409</td>\n",
       "      <td>0.836245</td>\n",
       "      <td>0.866946</td>\n",
       "      <td>0.888203</td>\n",
       "      <td>0.924473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039303</td>\n",
       "      <td>0.019833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106759</td>\n",
       "      <td>0.123894</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.108247</td>\n",
       "      <td>0.094326</td>\n",
       "      <td>0.101681</td>\n",
       "      <td>0.398784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.593047</td>\n",
       "      <td>0.843844</td>\n",
       "      <td>0.834038</td>\n",
       "      <td>0.833590</td>\n",
       "      <td>0.843089</td>\n",
       "      <td>0.869607</td>\n",
       "      <td>0.896440</td>\n",
       "      <td>0.927509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116361</td>\n",
       "      <td>0.141522</td>\n",
       "      <td>0.123883</td>\n",
       "      <td>0.120471</td>\n",
       "      <td>0.107461</td>\n",
       "      <td>0.108718</td>\n",
       "      <td>0.280851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.646995</td>\n",
       "      <td>0.925464</td>\n",
       "      <td>0.925004</td>\n",
       "      <td>0.924958</td>\n",
       "      <td>0.861715</td>\n",
       "      <td>0.919039</td>\n",
       "      <td>0.941759</td>\n",
       "      <td>0.966469</td>\n",
       "      <td>0.747649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728003</td>\n",
       "      <td>0.721305</td>\n",
       "      <td>0.697463</td>\n",
       "      <td>0.315587</td>\n",
       "      <td>0.328616</td>\n",
       "      <td>0.322843</td>\n",
       "      <td>0.320618</td>\n",
       "      <td>0.142697</td>\n",
       "      <td>0.147689</td>\n",
       "      <td>0.330699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.508972</td>\n",
       "      <td>0.867887</td>\n",
       "      <td>0.865675</td>\n",
       "      <td>0.862664</td>\n",
       "      <td>0.851582</td>\n",
       "      <td>0.892195</td>\n",
       "      <td>0.895863</td>\n",
       "      <td>0.931611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021154</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125745</td>\n",
       "      <td>0.140699</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>0.124584</td>\n",
       "      <td>0.105344</td>\n",
       "      <td>0.104563</td>\n",
       "      <td>0.305167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73487</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.462685</td>\n",
       "      <td>0.792289</td>\n",
       "      <td>0.774293</td>\n",
       "      <td>0.770557</td>\n",
       "      <td>0.825029</td>\n",
       "      <td>0.843009</td>\n",
       "      <td>0.853690</td>\n",
       "      <td>0.898127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048507</td>\n",
       "      <td>0.037961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038092</td>\n",
       "      <td>0.066916</td>\n",
       "      <td>0.052391</td>\n",
       "      <td>0.047050</td>\n",
       "      <td>0.065959</td>\n",
       "      <td>0.068696</td>\n",
       "      <td>0.055015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73488</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.606655</td>\n",
       "      <td>0.846074</td>\n",
       "      <td>0.843262</td>\n",
       "      <td>0.841001</td>\n",
       "      <td>0.855176</td>\n",
       "      <td>0.876780</td>\n",
       "      <td>0.889427</td>\n",
       "      <td>0.922339</td>\n",
       "      <td>0.415645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270086</td>\n",
       "      <td>0.210162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086322</td>\n",
       "      <td>0.134423</td>\n",
       "      <td>0.121065</td>\n",
       "      <td>0.113201</td>\n",
       "      <td>0.120419</td>\n",
       "      <td>0.122110</td>\n",
       "      <td>0.323100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73489</th>\n",
       "      <td>C</td>\n",
       "      <td>0.711481</td>\n",
       "      <td>0.943771</td>\n",
       "      <td>0.939821</td>\n",
       "      <td>0.938537</td>\n",
       "      <td>0.876509</td>\n",
       "      <td>0.930272</td>\n",
       "      <td>0.956541</td>\n",
       "      <td>0.972969</td>\n",
       "      <td>0.764108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758822</td>\n",
       "      <td>0.755744</td>\n",
       "      <td>0.753714</td>\n",
       "      <td>0.409456</td>\n",
       "      <td>0.429922</td>\n",
       "      <td>0.414165</td>\n",
       "      <td>0.416328</td>\n",
       "      <td>0.190574</td>\n",
       "      <td>0.195310</td>\n",
       "      <td>0.410030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73490</th>\n",
       "      <td>B</td>\n",
       "      <td>0.732800</td>\n",
       "      <td>0.929611</td>\n",
       "      <td>0.926733</td>\n",
       "      <td>0.925886</td>\n",
       "      <td>0.874946</td>\n",
       "      <td>0.912685</td>\n",
       "      <td>0.951605</td>\n",
       "      <td>0.973103</td>\n",
       "      <td>0.703855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713355</td>\n",
       "      <td>0.717663</td>\n",
       "      <td>0.713111</td>\n",
       "      <td>0.370843</td>\n",
       "      <td>0.395401</td>\n",
       "      <td>0.374104</td>\n",
       "      <td>0.366209</td>\n",
       "      <td>0.188689</td>\n",
       "      <td>0.204119</td>\n",
       "      <td>0.145593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73491</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.326030</td>\n",
       "      <td>0.752903</td>\n",
       "      <td>0.743557</td>\n",
       "      <td>0.746675</td>\n",
       "      <td>0.794974</td>\n",
       "      <td>0.820455</td>\n",
       "      <td>0.822926</td>\n",
       "      <td>0.881240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027039</td>\n",
       "      <td>0.041070</td>\n",
       "      <td>0.033653</td>\n",
       "      <td>0.033708</td>\n",
       "      <td>0.054619</td>\n",
       "      <td>0.056536</td>\n",
       "      <td>0.434650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73492 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lab  TOTUSJH_var  TOTBSQ_max  TOTBSQ_quadratic_weighted_average  \\\n",
       "0      NF     0.703435    0.886776                           0.880515   \n",
       "1      NF     0.536687    0.837925                           0.829410   \n",
       "2      NF     0.593047    0.843844                           0.834038   \n",
       "3      NF     0.646995    0.925464                           0.925004   \n",
       "4      NF     0.508972    0.867887                           0.865675   \n",
       "...    ..          ...         ...                                ...   \n",
       "73487  NF     0.462685    0.792289                           0.774293   \n",
       "73488  NF     0.606655    0.846074                           0.843262   \n",
       "73489   C     0.711481    0.943771                           0.939821   \n",
       "73490   B     0.732800    0.929611                           0.926733   \n",
       "73491  NF     0.326030    0.752903                           0.743557   \n",
       "\n",
       "       TOTBSQ_last_value  TOTPOT_stddev  TOTPOT_linear_weighted_average  \\\n",
       "0               0.883499       0.856244                        0.891949   \n",
       "1               0.828409       0.836245                        0.866946   \n",
       "2               0.833590       0.843089                        0.869607   \n",
       "3               0.924958       0.861715                        0.919039   \n",
       "4               0.862664       0.851582                        0.892195   \n",
       "...                  ...            ...                             ...   \n",
       "73487           0.770557       0.825029                        0.843009   \n",
       "73488           0.841001       0.855176                        0.876780   \n",
       "73489           0.938537       0.876509                        0.930272   \n",
       "73490           0.925886       0.874946                        0.912685   \n",
       "73491           0.746675       0.794974                        0.820455   \n",
       "\n",
       "       TOTUSJZ_mean  USFLUX_quadratic_weighted_average  R_VALUE_median  ...  \\\n",
       "0          0.930756                           0.950338        0.000000  ...   \n",
       "1          0.888203                           0.924473        0.000000  ...   \n",
       "2          0.896440                           0.927509        0.000000  ...   \n",
       "3          0.941759                           0.966469        0.747649  ...   \n",
       "4          0.895863                           0.931611        0.000000  ...   \n",
       "...             ...                                ...             ...  ...   \n",
       "73487      0.853690                           0.898127        0.000000  ...   \n",
       "73488      0.889427                           0.922339        0.415645  ...   \n",
       "73489      0.956541                           0.972969        0.764108  ...   \n",
       "73490      0.951605                           0.973103        0.703855  ...   \n",
       "73491      0.822926                           0.881240        0.000000  ...   \n",
       "\n",
       "       R_VALUE_linear_weighted_average  R_VALUE_quadratic_weighted_average  \\\n",
       "0                             0.086008                            0.095111   \n",
       "1                             0.039303                            0.019833   \n",
       "2                             0.000000                            0.000000   \n",
       "3                             0.728003                            0.721305   \n",
       "4                             0.021154                            0.009999   \n",
       "...                                ...                                 ...   \n",
       "73487                         0.048507                            0.037961   \n",
       "73488                         0.270086                            0.210162   \n",
       "73489                         0.758822                            0.755744   \n",
       "73490                         0.713355                            0.717663   \n",
       "73491                         0.000000                            0.000000   \n",
       "\n",
       "       R_VALUE_last_value  TOTUSJH_min  TOTUSJH_max  \\\n",
       "0                0.371876     0.238758     0.275990   \n",
       "1                0.000000     0.106759     0.123894   \n",
       "2                0.000000     0.116361     0.141522   \n",
       "3                0.697463     0.315587     0.328616   \n",
       "4                0.000000     0.125745     0.140699   \n",
       "...                   ...          ...          ...   \n",
       "73487            0.000000     0.038092     0.066916   \n",
       "73488            0.000000     0.086322     0.134423   \n",
       "73489            0.753714     0.409456     0.429922   \n",
       "73490            0.713111     0.370843     0.395401   \n",
       "73491            0.000000     0.027039     0.041070   \n",
       "\n",
       "       TOTUSJH_quadratic_weighted_average  TOTUSJH_last_value  \\\n",
       "0                                0.250146            0.271143   \n",
       "1                                0.109375            0.108247   \n",
       "2                                0.123883            0.120471   \n",
       "3                                0.322843            0.320618   \n",
       "4                                0.132812            0.124584   \n",
       "...                                   ...                 ...   \n",
       "73487                            0.052391            0.047050   \n",
       "73488                            0.121065            0.113201   \n",
       "73489                            0.414165            0.416328   \n",
       "73490                            0.374104            0.366209   \n",
       "73491                            0.033653            0.033708   \n",
       "\n",
       "       ABSNJZH_dderivative_stddev  ABSNJZH_avg_mono_increase_slope        id  \n",
       "0                        0.143979                         0.147190  0.294529  \n",
       "1                        0.094326                         0.101681  0.398784  \n",
       "2                        0.107461                         0.108718  0.280851  \n",
       "3                        0.142697                         0.147689  0.330699  \n",
       "4                        0.105344                         0.104563  0.305167  \n",
       "...                           ...                              ...       ...  \n",
       "73487                    0.065959                         0.068696  0.055015  \n",
       "73488                    0.120419                         0.122110  0.323100  \n",
       "73489                    0.190574                         0.195310  0.410030  \n",
       "73490                    0.188689                         0.204119  0.145593  \n",
       "73491                    0.054619                         0.056536  0.434650  \n",
       "\n",
       "[73492 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #----------------------------------------------\n",
    "    x_labels = abt_copy['lab']\n",
    "    x_features = abt_copy.drop(['lab'], axis = 1)\n",
    "    x_model_features = SelectFromModel(max_features = numFeat, estimator = ExtraTreesClassifier(n_estimators = 75)).fit(x_features, x_labels)\n",
    "    x_selected_features = x_features.loc[:, x_model_features.get_support()]\n",
    "    x_train = pd.concat([x_labels, x_selected_features], axis = 1)\n",
    "    x_train\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6332868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lab</th>\n",
       "      <th>TOTUSJH_var</th>\n",
       "      <th>TOTBSQ_max</th>\n",
       "      <th>TOTBSQ_quadratic_weighted_average</th>\n",
       "      <th>TOTBSQ_last_value</th>\n",
       "      <th>TOTPOT_stddev</th>\n",
       "      <th>TOTPOT_linear_weighted_average</th>\n",
       "      <th>TOTUSJZ_mean</th>\n",
       "      <th>USFLUX_quadratic_weighted_average</th>\n",
       "      <th>R_VALUE_median</th>\n",
       "      <th>...</th>\n",
       "      <th>R_VALUE_linear_weighted_average</th>\n",
       "      <th>R_VALUE_quadratic_weighted_average</th>\n",
       "      <th>R_VALUE_last_value</th>\n",
       "      <th>TOTUSJH_min</th>\n",
       "      <th>TOTUSJH_max</th>\n",
       "      <th>TOTUSJH_quadratic_weighted_average</th>\n",
       "      <th>TOTUSJH_last_value</th>\n",
       "      <th>ABSNJZH_dderivative_stddev</th>\n",
       "      <th>ABSNJZH_avg_mono_increase_slope</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.625650</td>\n",
       "      <td>0.910396</td>\n",
       "      <td>0.910168</td>\n",
       "      <td>0.910402</td>\n",
       "      <td>0.868978</td>\n",
       "      <td>0.918002</td>\n",
       "      <td>0.932597</td>\n",
       "      <td>0.951585</td>\n",
       "      <td>0.575157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444758</td>\n",
       "      <td>0.410609</td>\n",
       "      <td>0.547147</td>\n",
       "      <td>0.248654</td>\n",
       "      <td>0.264179</td>\n",
       "      <td>0.255972</td>\n",
       "      <td>0.253303</td>\n",
       "      <td>0.156771</td>\n",
       "      <td>0.175954</td>\n",
       "      <td>0.578116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.524445</td>\n",
       "      <td>0.859605</td>\n",
       "      <td>0.854401</td>\n",
       "      <td>0.853716</td>\n",
       "      <td>0.842308</td>\n",
       "      <td>0.878799</td>\n",
       "      <td>0.906536</td>\n",
       "      <td>0.939085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148949</td>\n",
       "      <td>0.161652</td>\n",
       "      <td>0.154432</td>\n",
       "      <td>0.152454</td>\n",
       "      <td>0.112890</td>\n",
       "      <td>0.121236</td>\n",
       "      <td>0.736474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.542118</td>\n",
       "      <td>0.863722</td>\n",
       "      <td>0.861191</td>\n",
       "      <td>0.858692</td>\n",
       "      <td>0.843777</td>\n",
       "      <td>0.882648</td>\n",
       "      <td>0.905747</td>\n",
       "      <td>0.935372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.013494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161101</td>\n",
       "      <td>0.174518</td>\n",
       "      <td>0.167077</td>\n",
       "      <td>0.163801</td>\n",
       "      <td>0.157069</td>\n",
       "      <td>0.163217</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.690086</td>\n",
       "      <td>0.913040</td>\n",
       "      <td>0.909206</td>\n",
       "      <td>0.907126</td>\n",
       "      <td>0.877817</td>\n",
       "      <td>0.909533</td>\n",
       "      <td>0.933567</td>\n",
       "      <td>0.959465</td>\n",
       "      <td>0.713647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712612</td>\n",
       "      <td>0.711166</td>\n",
       "      <td>0.701873</td>\n",
       "      <td>0.263200</td>\n",
       "      <td>0.292981</td>\n",
       "      <td>0.270127</td>\n",
       "      <td>0.260010</td>\n",
       "      <td>0.136500</td>\n",
       "      <td>0.138083</td>\n",
       "      <td>0.858359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.447053</td>\n",
       "      <td>0.842784</td>\n",
       "      <td>0.838926</td>\n",
       "      <td>0.836830</td>\n",
       "      <td>0.843102</td>\n",
       "      <td>0.873573</td>\n",
       "      <td>0.877655</td>\n",
       "      <td>0.921374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117955</td>\n",
       "      <td>0.135080</td>\n",
       "      <td>0.310076</td>\n",
       "      <td>0.091531</td>\n",
       "      <td>0.104345</td>\n",
       "      <td>0.097687</td>\n",
       "      <td>0.094275</td>\n",
       "      <td>0.087940</td>\n",
       "      <td>0.089760</td>\n",
       "      <td>0.589970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88552</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.636863</td>\n",
       "      <td>0.870733</td>\n",
       "      <td>0.863351</td>\n",
       "      <td>0.861416</td>\n",
       "      <td>0.857414</td>\n",
       "      <td>0.883171</td>\n",
       "      <td>0.915647</td>\n",
       "      <td>0.940915</td>\n",
       "      <td>0.356644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163147</td>\n",
       "      <td>0.105829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181140</td>\n",
       "      <td>0.205758</td>\n",
       "      <td>0.185497</td>\n",
       "      <td>0.180158</td>\n",
       "      <td>0.137763</td>\n",
       "      <td>0.138010</td>\n",
       "      <td>0.941337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88553</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.342742</td>\n",
       "      <td>0.788695</td>\n",
       "      <td>0.785622</td>\n",
       "      <td>0.787742</td>\n",
       "      <td>0.806316</td>\n",
       "      <td>0.844060</td>\n",
       "      <td>0.848843</td>\n",
       "      <td>0.902201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049787</td>\n",
       "      <td>0.058479</td>\n",
       "      <td>0.055035</td>\n",
       "      <td>0.057382</td>\n",
       "      <td>0.070615</td>\n",
       "      <td>0.073404</td>\n",
       "      <td>0.658663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88554</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.401249</td>\n",
       "      <td>0.787746</td>\n",
       "      <td>0.775761</td>\n",
       "      <td>0.771510</td>\n",
       "      <td>0.815244</td>\n",
       "      <td>0.835070</td>\n",
       "      <td>0.847389</td>\n",
       "      <td>0.895124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009245</td>\n",
       "      <td>0.005289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>0.062495</td>\n",
       "      <td>0.053349</td>\n",
       "      <td>0.053345</td>\n",
       "      <td>0.078176</td>\n",
       "      <td>0.079521</td>\n",
       "      <td>0.800304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88555</th>\n",
       "      <td>NF</td>\n",
       "      <td>0.537470</td>\n",
       "      <td>0.834092</td>\n",
       "      <td>0.831255</td>\n",
       "      <td>0.829807</td>\n",
       "      <td>0.828145</td>\n",
       "      <td>0.866177</td>\n",
       "      <td>0.884763</td>\n",
       "      <td>0.926024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094972</td>\n",
       "      <td>0.115870</td>\n",
       "      <td>0.107972</td>\n",
       "      <td>0.105768</td>\n",
       "      <td>0.093569</td>\n",
       "      <td>0.097608</td>\n",
       "      <td>0.881155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88556</th>\n",
       "      <td>B</td>\n",
       "      <td>0.678505</td>\n",
       "      <td>0.918488</td>\n",
       "      <td>0.916336</td>\n",
       "      <td>0.918494</td>\n",
       "      <td>0.865038</td>\n",
       "      <td>0.911575</td>\n",
       "      <td>0.947886</td>\n",
       "      <td>0.964541</td>\n",
       "      <td>0.694981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690656</td>\n",
       "      <td>0.688708</td>\n",
       "      <td>0.684747</td>\n",
       "      <td>0.344644</td>\n",
       "      <td>0.365650</td>\n",
       "      <td>0.359726</td>\n",
       "      <td>0.366120</td>\n",
       "      <td>0.174209</td>\n",
       "      <td>0.184904</td>\n",
       "      <td>0.947720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88557 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lab  TOTUSJH_var  TOTBSQ_max  TOTBSQ_quadratic_weighted_average  \\\n",
       "0      NF     0.625650    0.910396                           0.910168   \n",
       "1      NF     0.524445    0.859605                           0.854401   \n",
       "2      NF     0.542118    0.863722                           0.861191   \n",
       "3      NF     0.690086    0.913040                           0.909206   \n",
       "4      NF     0.447053    0.842784                           0.838926   \n",
       "...    ..          ...         ...                                ...   \n",
       "88552  NF     0.636863    0.870733                           0.863351   \n",
       "88553  NF     0.342742    0.788695                           0.785622   \n",
       "88554  NF     0.401249    0.787746                           0.775761   \n",
       "88555  NF     0.537470    0.834092                           0.831255   \n",
       "88556   B     0.678505    0.918488                           0.916336   \n",
       "\n",
       "       TOTBSQ_last_value  TOTPOT_stddev  TOTPOT_linear_weighted_average  \\\n",
       "0               0.910402       0.868978                        0.918002   \n",
       "1               0.853716       0.842308                        0.878799   \n",
       "2               0.858692       0.843777                        0.882648   \n",
       "3               0.907126       0.877817                        0.909533   \n",
       "4               0.836830       0.843102                        0.873573   \n",
       "...                  ...            ...                             ...   \n",
       "88552           0.861416       0.857414                        0.883171   \n",
       "88553           0.787742       0.806316                        0.844060   \n",
       "88554           0.771510       0.815244                        0.835070   \n",
       "88555           0.829807       0.828145                        0.866177   \n",
       "88556           0.918494       0.865038                        0.911575   \n",
       "\n",
       "       TOTUSJZ_mean  USFLUX_quadratic_weighted_average  R_VALUE_median  ...  \\\n",
       "0          0.932597                           0.951585        0.575157  ...   \n",
       "1          0.906536                           0.939085        0.000000  ...   \n",
       "2          0.905747                           0.935372        0.000000  ...   \n",
       "3          0.933567                           0.959465        0.713647  ...   \n",
       "4          0.877655                           0.921374        0.000000  ...   \n",
       "...             ...                                ...             ...  ...   \n",
       "88552      0.915647                           0.940915        0.356644  ...   \n",
       "88553      0.848843                           0.902201        0.000000  ...   \n",
       "88554      0.847389                           0.895124        0.000000  ...   \n",
       "88555      0.884763                           0.926024        0.000000  ...   \n",
       "88556      0.947886                           0.964541        0.694981  ...   \n",
       "\n",
       "       R_VALUE_linear_weighted_average  R_VALUE_quadratic_weighted_average  \\\n",
       "0                             0.444758                            0.410609   \n",
       "1                             0.000000                            0.000000   \n",
       "2                             0.030981                            0.013494   \n",
       "3                             0.712612                            0.711166   \n",
       "4                             0.117955                            0.135080   \n",
       "...                                ...                                 ...   \n",
       "88552                         0.163147                            0.105829   \n",
       "88553                         0.000000                            0.000000   \n",
       "88554                         0.009245                            0.005289   \n",
       "88555                         0.000000                            0.000000   \n",
       "88556                         0.690656                            0.688708   \n",
       "\n",
       "       R_VALUE_last_value  TOTUSJH_min  TOTUSJH_max  \\\n",
       "0                0.547147     0.248654     0.264179   \n",
       "1                0.000000     0.148949     0.161652   \n",
       "2                0.000000     0.161101     0.174518   \n",
       "3                0.701873     0.263200     0.292981   \n",
       "4                0.310076     0.091531     0.104345   \n",
       "...                   ...          ...          ...   \n",
       "88552            0.000000     0.181140     0.205758   \n",
       "88553            0.000000     0.049787     0.058479   \n",
       "88554            0.000000     0.047382     0.062495   \n",
       "88555            0.000000     0.094972     0.115870   \n",
       "88556            0.684747     0.344644     0.365650   \n",
       "\n",
       "       TOTUSJH_quadratic_weighted_average  TOTUSJH_last_value  \\\n",
       "0                                0.255972            0.253303   \n",
       "1                                0.154432            0.152454   \n",
       "2                                0.167077            0.163801   \n",
       "3                                0.270127            0.260010   \n",
       "4                                0.097687            0.094275   \n",
       "...                                   ...                 ...   \n",
       "88552                            0.185497            0.180158   \n",
       "88553                            0.055035            0.057382   \n",
       "88554                            0.053349            0.053345   \n",
       "88555                            0.107972            0.105768   \n",
       "88556                            0.359726            0.366120   \n",
       "\n",
       "       ABSNJZH_dderivative_stddev  ABSNJZH_avg_mono_increase_slope        id  \n",
       "0                        0.156771                         0.175954  0.578116  \n",
       "1                        0.112890                         0.121236  0.736474  \n",
       "2                        0.157069                         0.163217  0.942857  \n",
       "3                        0.136500                         0.138083  0.858359  \n",
       "4                        0.087940                         0.089760  0.589970  \n",
       "...                           ...                              ...       ...  \n",
       "88552                    0.137763                         0.138010  0.941337  \n",
       "88553                    0.070615                         0.073404  0.658663  \n",
       "88554                    0.078176                         0.079521  0.800304  \n",
       "88555                    0.093569                         0.097608  0.881155  \n",
       "88556                    0.174209                         0.184904  0.947720  \n",
       "\n",
       "[88557 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_labels = abt2_copy['lab']\n",
    "x_test_features = abt2_copy.drop(['lab'], axis = 1)\n",
    "x_test_selected = x_test_features.loc[:, x_model_features.get_support()]\n",
    "x_test = pd.concat([x_test_labels, x_test_selected], axis = 1)\n",
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39118a80",
   "metadata": {},
   "source": [
    "---\n",
    "### Q3 (5 points)\n",
    "\n",
    "Now that you have three different datasets, you need to convert them each to a binary classification problem datase or dichotomize the training and testing data. Lucky for you, a method has already been provided to do this. All you need to do is apply it to teach of the `DataFrame`s you constructed with the feature selected training and testing data from the exmpale, Q1, and Q2.\n",
    "\n",
    "**Note:** You might want to put the training and testing tuples you get from the call to the dichotomize method into seperate training and testing lists. Then you can loop over them later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67f8b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #----------------------------------------------\n",
    "    train = [df_train_set1.copy(), l_trainset.copy(), x_train.copy()]\n",
    "    test = [df_test_set1.copy(), l_testset.copy(), x_test.copy()]\n",
    "    di_train_x1 = []\n",
    "    di_train_y1 = []\n",
    "    di_test_x2 = []\n",
    "    di_test_y2 = []\n",
    "    for set1 in train:\n",
    "        x1, y1 = dichotomize_X_y(set1)\n",
    "        di_train_x1.append(x1)\n",
    "        di_train_y1.append(y1)\n",
    "    for set2 in test:\n",
    "        x2, y2 = dichotomize_X_y(set2)\n",
    "        di_test_x2.append(x2)\n",
    "        di_test_y2.append(y2)\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266a5bc",
   "metadata": {},
   "source": [
    "---\n",
    "### Q4 (10 points)\n",
    "\n",
    "Now that you have your data setup, you will be constructing an [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on each one of the different datasets. For this exercise default regularization parameter `C` value of 1.0, the default `kernel` type of `rbf`, and the default setting of the kernel coefficient `gamma` for the `rbf` kernel. You should, however, set the `class_weight` to `balanced` when you construct your models. This way the regularization parameter is adjusted for each class in proportion the occurrence of that class in the dataset.\n",
    "\n",
    "You should train the model on your training data, then test it on the testing data with the same set of selected descriptive features. You will then calculate both the TSS and HSS scores and print them out.\n",
    "\n",
    "**Note:** for more information on what the `C` and `gamma` parameters do on the `rbf` kernel see the [RBF SVM parameters](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html) documentation. We won't be tuning these values in this question, but it is genearally accepted that tuning should be done to find the best performing model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6380e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = ['F-Val', 'FromLasso', 'FromForest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f99430b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN=76129\tFP=11027\tFN=76\tTP=1325\n",
      "F-Val\n",
      "TSS: 0.8192327710296818\n",
      "HSS: 0.1690723900997794\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e94cd4e0b8b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'balanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi_train_x1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdi_train_y1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi_test_x2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mt_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_tss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi_test_y2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mh_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_hss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi_test_y2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             X = check_array(X, accept_sparse='csr', dtype=np.float64,\n\u001b[0m\u001b[1;32m    475\u001b[0m                             order=\"C\", accept_large_sparse=False)\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "    #----------------------------------------------\n",
    "    for ind in range(len(selected_labels)):\n",
    "        classifier = SVC(class_weight = 'balanced')\n",
    "        classifier.fit(di_train_x1[ind],di_train_y1[ind])\n",
    "        y_pred = classifier.predict(di_test_x2[ind])\n",
    "        t_score = calc_tss(di_test_y2[ind], y_pred)\n",
    "        h_score = calc_hss(di_test_y2[ind], y_pred)\n",
    "        print(selected_labels[ind])\n",
    "        print(f\"TSS: {t_score}\")\n",
    "        print(f\"HSS: {h_score}\")\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795526d",
   "metadata": {},
   "source": [
    "---\n",
    "### Q5 (15 points)\n",
    "\n",
    "After training and testing the SVC model on the above dataset, you will likely see that this process is quite time consuming. This is because the algorithm needs evaluate all the instances in the training dataset to find instances that can be used as points in a separating hyperplane between the samples of different classes.  \n",
    "\n",
    "In order to speed this process, lets reduce the number of samples in the dataset through undersampling the classes like we did before. Unlike was done before, where we just pick some random sample of instances in the various classes, we will be performing some data informed under sampling using the [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) clustering algorithm.\n",
    "\n",
    "So, what I want you to do is construct a function (I have started it below) that performs the following:\n",
    "\n",
    "* Groups the data by the `lab` column and finds the size of the smallest group.\n",
    "\n",
    "* For every group of samples in the dataset that is not the smallest group, you will use the KMeans algorithm to cluster the samples into `K` clusters. The size of `K` is the size of the smallest group.  **Note:** You need to make sure you are only using the descriptive features when doing this, so drop the label column from each group.\n",
    "\n",
    "* Once you have the `K` clusters, you will get the cluster centers by using the `cluster_centers_` attribute of your Kmeans object. This attribute is a `(n_clusters, n_features)` array of values. These will be the new set of samples of descriptive features for the class you are processing. You should construct a DataFrame with these and add a label column with your class label for each one of these samples.\n",
    "\n",
    "* The samples for the smalles class group from the original dataset will be the samples you return for that class. \n",
    "\n",
    "* You will need to concatenate all of the results into one DataFrame and return it at the end of the function.\n",
    "\n",
    "Once you have completed the function, you need to apply it to each of your three training sets (the ones that have not had the dicotimize process applied, I hope you kept a copy). Then you will apply the dicotomize process to the sampled training sets and place them into a list for use in the next problem.\n",
    "\n",
    "**Note:** By training our models on representations of the real data instead of the acutal measurements, we are building a type of surrogate model. By doing so, we can approximate how our model might behave when trained with the true data, but can test several different settings much faster than what we otherwise would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8acd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_under_sample_clust(data:DataFrame)->DataFrame:\n",
    "    #----------------------------------------------\n",
    "    thetemp = data.copy()\n",
    "    theres = pd.DataFrame()\n",
    "    freq = pd.DataFrame(thetemp['lab'].value_counts())\n",
    "    indices = freq.index.tolist()\n",
    "    min_val_ind = freq.idxmin()[0]\n",
    "    indices.remove(min_val_ind)\n",
    "    min_v = freq['lab'][min_val_ind]\n",
    "    desc_feats = thetemp.columns[1:]\n",
    "    theres = pd.concat([theres, thetemp[thetemp['lab'] == min_val_ind]])\n",
    "    for ind in indices:\n",
    "        classifier = KMeans(n_clusters=min_v)\n",
    "        classifier.fit(thetemp[thetemp['lab'] == ind].drop(['lab'], axis = 1))\n",
    "        thetemp2 = pd.DataFrame(classifier.cluster_centers_, columns=desc_feats)\n",
    "        thetemp2['lab'] = ind\n",
    "        theres = pd.concat([theres, thetemp2]) \n",
    "    theres.reset_index(inplace=True, drop=True)\n",
    "    return theres\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #----------------------------------------------\n",
    "    for frame in train:\n",
    "        frame = perform_under_sample_clust(frame)\n",
    "    us_di_train_x1 = []\n",
    "    us_di_train_y1 = []\n",
    "    for frame in train:\n",
    "        x1, y1 = dichotomize_X_y(frame)\n",
    "        us_di_train_x1.append(x1)\n",
    "        us_di_train_y1.append(y1)\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050f3ff",
   "metadata": {},
   "source": [
    "---\n",
    "### 6 (10 points)\n",
    "\n",
    "In question 5 we produced datasets that approxumate what our real data looks like. By training our models on these representations of the real data instead of the acutal measurements, we are building a type of surrogate model. In doing so, we can approximate how our model might behave when trained with the true data, but we obtain a major advantage in that we can test several different settings much faster than what we otherwise would if using the true dataset. We can then use these surrogate results to find a range of the hyperparameters that we might wish to investigate using the true input data.\n",
    "\n",
    "For this question, you will again train your models on the three different feature selected data. However, instead of the full partition 1 training datasets, you will be using the sampling with KMeans training datasets you constructed in Q5. \n",
    "\n",
    "You will again be constructing an [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on each one of the different datasets. Like before, you should set the `class_weight` to `balanced` when you construct your models. Unlike the previous question where you traind the models, this time you will be asked to evaluate several different settings for the `kernel`, the regularization parameter `C`, and kernel coefficient `gamma`. **Note:** The `gamma` paramter is only utilized on the ‘rbf’, ‘poly’ and ‘sigmoid’ kernels, so there is no reason to evaluate multiple settings for the `linear` kernel. I have listed the settings of each parameter in a code block below. \n",
    "\n",
    "For each of the settings, you should train the model on your training data, then test it on the testing data with the same set of selected descriptive features. You will then calculate both the TSS and HSS scores and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_labels = ['F-Val', 'FromLasso', 'FromForest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d17f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['linear', 'poly', 'rbf']\n",
    "c_vals = [ 0.5, 1.0]\n",
    "gamma_vals = [0.5, 1, 10]\n",
    "temp = [kernel, c_vals, gamma_vals]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #----------------------------------------------\n",
    "    for ind in range(len(selected_labels)):\n",
    "        print(selected_labels[ind], \"- - - - - - - - - -- - - - -\")\n",
    "        for kernel, c_value, gamma_value in params:\n",
    "                classifier = SVC(class_weight = 'balanced', gamma=gamma_value, C=c_value, kernel=kernel)\n",
    "                classifier.fit(us_di_train_x1[ind],di_train_y1[ind])\n",
    "                y_pred = classifier.predict(di_test_x2[ind])\n",
    "                t_score = calc_tss(di_test_y2[ind], y_pred)\n",
    "                h_score = calc_hss(di_test_y2[ind], y_pred)\n",
    "                print(f\"TSS: {t_score}\")\n",
    "                print(f\"HSS: {h_score}\")\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49462023",
   "metadata": {},
   "source": [
    "---\n",
    "### 7 (10 points)\n",
    "\n",
    "Results above were able to find some combinations of hyperparamters and datasets that work fairly well for our problem. But the question remains, can we do better?\n",
    "\n",
    "Maybe one way to improve our results would be to elinate the easy to classify instances from our dataset and only focus our efforts on the more difficult ones. If you recall from our data preparation there was a feature in our dataset that we could use to easily distinguish between a rather large percentage of `flare` and `non-flare` data. This feature was `R_VALUE_median`, but we don't know what value to use to filter off part of our data.\n",
    "\n",
    "So, for this question, let's plot and see where a good cutoff might be. To do this, let's use the seaborn [ecdfplot](https://seaborn.pydata.org/generated/seaborn.ecdfplot.html#seaborn.ecdfplot) or the cumulative distribution function plot.  Your input will be the original analytics base table of partition one.  You should set the `x` axis to `R_VALUE_median`, and set the `hue` to `lab`.\n",
    "\n",
    "After plotting this, you will see that around 0.5 we begin to see some instances of the `M` and around 0.7 we begin to see some instances of the `X` class flares in our dataset. So, use 0.5 as a threshold value to filter out all of the instances that fall below this threshold from our training data. Construct a copy of the original partition 1 data with this applied.\n",
    "\n",
    "You can then verify this using the seaborn [kdeplot](https://seaborn.pydata.org/generated/seaborn.kdeplot.html#seaborn.kdeplot) using the new filtered data as input, setting `x` to `R_VALUE_median` again, and setting `hue` to `lab` again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #----------------------------------------------\n",
    "    sns.ecdfplot(abt, hue='lab', x='R_VALUE_median')\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe3e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_abt = abt[abt['R_VALUE_median'] >= 0.5].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0dba1",
   "metadata": {},
   "source": [
    "---\n",
    "### 8 (10 points)\n",
    "\n",
    "For this question, you will utilize the filtered analytics base table you constructed in the previous question.  You should:\n",
    "\n",
    "* Repeat the feature selection I did for you in the  example using the [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection), and scoring function [scikit-learn f_classif](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif).\n",
    "\n",
    "* Repeat the feature selection from Q1 using the [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection), and utilizing the [LassoLars](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars) as your `estimator`.  You should also set the `max_features` to the number of features we are going to select (20 features).\n",
    "\n",
    "* Repeat the feature selection from Q2 using [SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel) class from [scikit-learn Univariate Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection). With a random forest model called [ExtraTressClssifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) as the `estimator`. The `n_estimators` of the random forest algorithm should be set to `75` when you construct it. The `max_features` of the SelectFromModel should be set to the number of features we are going to select (20 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0f0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #----------------------------------------------\n",
    "numFeat = 20\n",
    "\n",
    "\n",
    "df_labels = filter_abt['lab'].copy()\n",
    "df_feats = filter_abt.copy().drop(['lab'], axis=1)\n",
    "df_test_labels = abt2['lab'].copy()\n",
    "df_test_feats = abt2.copy().drop(['lab'], axis=1)\n",
    "feats1 = SelectKBest(f_classif, k=numFeat).fit(df_feats, df_labels)\n",
    "df_selected_feats1 = df_feats.loc[:, feats1.get_support()]\n",
    "df_train_set1 = pd.concat([df_labels, df_selected_feats1], axis=1)\n",
    "df_test_selected_feats1 = df_test_feats.loc[:, feats1.get_support()]\n",
    "df_test_set1 = pd.concat([df_test_labels, df_test_selected_feats1], axis=1)\n",
    "#----------------------------------------------\n",
    "l_labs = filter_abt['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "l_feats = filter_abt.drop(['lab'], axis =1)\n",
    "l_df_feats = SelectFromModel(max_features = numFeat, estimator = LassoLars( alpha = 0, eps =1)).fit(l_feats, l_labs)\n",
    "l_train = l_feats.loc[:, l_df_feats.get_support()]\n",
    "l_trainset = pd.concat([filter_abt['lab'], l_train], axis = 1)\n",
    "#----------------------------------------------\n",
    "l2_labs = abt2_copy['lab'].map({'NF':-1, 'B':-0.5, 'C':0, 'M':0.5, 'X':1})\n",
    "l2_feats = abt2_copy.drop(['lab'], axis =1)\n",
    "l_df_testfeats = l2_feats.loc[:, l_df_feats.get_support()]\n",
    "l_test = l_feats.loc[:, l_df_feats.get_support()]\n",
    "l_test_set = pd.concat([abt2['lab'], l_test], axis = 1)\n",
    "#----------------------------------------------\n",
    "x_labels = filter_abt['lab'].copy()\n",
    "x_features = filter_abt.copy().drop(['lab'], axis = 1)\n",
    "x_model_features = SelectFromModel(max_features = numFeat, estimator = ExtraTreesClassifier(n_estimators = 75)).fit(x_features, x_labels)\n",
    "x_selected_features = x_features.loc[:, x_model_features.get_support()]\n",
    "x_train = pd.concat([x_labels, x_selected_features], axis = 1)\n",
    "#----------------------------------------------\n",
    "x_test_labels = abt2_copy['lab']\n",
    "x_test_features = abt2_copy.drop(['lab'], axis = 1)\n",
    "x_test_selected = x_test_features.loc[:, x_model_features.get_support()]\n",
    "x_test = pd.concat([x_test_labels, x_test_selected], axis = 1)\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbd0bc8",
   "metadata": {},
   "source": [
    "---\n",
    "### Q9 (10 points)\n",
    "\n",
    "Using the training and testing datsets you constructed in the previous question after performing feature selection on the filtered partition 1 analytics base table. You now need to perform the sampling on the training data using the function you made in Q5.\n",
    "\n",
    "Then you should convert each of the new training and testing datasets to a binary classification problem datase or dichotomize the training and testing data like you did in Q3. Lucky for you, a method has already been provided to do this. All you need to do is apply it to teach of the `DataFrame`s you constructed with the feature selected training and testing data.\n",
    "\n",
    "**Note:** You might want to put the training and testing tuples you get from the call to the dichotomize method into seperate training and testing lists. Then you can loop over them later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a2af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #----------------------------------------------\n",
    "    train = [df_train_set1.copy(), l_trainset.copy(), x_train.copy()]\n",
    "    test = [df_test_set1.copy(), l_testset.copy(), x_test.copy()]\n",
    "    #undersampling\n",
    "    for frame in train:\n",
    "        frame = perform_under_sample_clust(frame)\n",
    "    di_test_x1 = []\n",
    "    di_test_y1 = []\n",
    "    di_train_x2 = []\n",
    "    di_train_y2 = []\n",
    "    for frame in train:\n",
    "        x1, y1 = dichotomize_X_y(frame)\n",
    "        di_train_x1.append(x1)\n",
    "        di_train_y1.append(y1)\n",
    "    for frame in test:\n",
    "        x2, y2 = dichotomize_X_y(frame)\n",
    "        di_test_x2.append(x2)\n",
    "        di_test_y2.append(y2)\n",
    "\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dab10a",
   "metadata": {},
   "source": [
    "---\n",
    "### Q10 (20 points)\n",
    "\n",
    "Like in Q6, this question will be utilizing the filtered and sampled datasets constructed in the previous question. For this question, you will again train your models on the three different feature selected data that had the instances below our thrshold filtered out and then had sampling by clustering performed on them. \n",
    "\n",
    "You will again be constructing an [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on each one of the different datasets. Like before, you should set the `class_weight` to `balanced` when you construct your models. You will again evaluate several different settings for the `kernel`, the regularization parameter `C`, and kernel coefficient `gamma`. **Note:** The `gamma` paramter is only utilized on the ‘rbf’, ‘poly’ and ‘sigmoid’ kernels, so there is no reason to evaluate multiple settings for the `linear` kernel. I have listed the settings of each parameter in a code block below. \n",
    "\n",
    "For each of the settings, you should train the model on your training data, then test it on the testing data with the same set of selected descriptive features. You will then calculate both the TSS and HSS scores and print them out. **Note:** The testing data has the samples in it that are below our threshold value, so you will first need to filter those out of the data you plan to pass to your model for testing. However, you still want those instances included in the calculation of the TSS and HSSS. So, your groud truth `lab` data should include all the instances in partition 2. You will need to concatenate a vector with all zeros in it to the match the labels you partitioned from the model testing data. \n",
    "\n",
    "Let's give you a representation of that:\n",
    "    \n",
    "    labels_from_data = [labels for samples > threshold] + [labels for samples <= threshold]\n",
    "    predict_labels = [labels from the model on > thrshold samples] + [0s the length of samples <= threshold]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "\n",
    "kernel = ['linear', 'poly', 'rbf']\n",
    "c_vals = [ 0.5, 1.0]\n",
    "gamma_vals = [0.5, 1, 10]\n",
    "temp = [kernel, c_vals, gamma_vals]\n",
    "params = list(itertools.product(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22707ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #----------------------------------------------\n",
    "    for ind in range(len(selected_labels)):\n",
    "        print(selected_labels[ind], \"- - - - - - - - - - - - - -\", end='\\n\\n')\n",
    "        for kernel, c_value, gamma_value in params:\n",
    "            classifier = SVC(class_weight = 'balanced', gamma=gamma_value, C=c_value, kernel=kernel)\n",
    "            classifier.fit(di_train_x1[ind],di_train_y1[ind])\n",
    "            y_pred = classifier.predict(di_test_x2[ind])\n",
    "            t_score = calc_tss(di_test_y2[ind], y_pred)\n",
    "            h_score = calc_hss(di_test_y2[ind], y_pred)\n",
    "            print(f\"TSS: {t_score}\")\n",
    "            print(f\"HSS: {h_score}\")\n",
    "    #----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee068b2",
   "metadata": {},
   "source": [
    "All of these results are getting unruely, we should maybe be saving them to do analysis on them too? Maybe I'll ask you to do that for the extra credit assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae63f37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
